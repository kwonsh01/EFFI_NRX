@misc{pruning,
  title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks}, 
  author={Jonathan Frankle and Michael Carbin},
  year={2019},
  eprint={1803.03635},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1803.03635}, 
}

@inproceedings{movement_pruning,
 author = {Sanh, Victor and Wolf, Thomas and Rush, Alexander},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {20378--20389},
 publisher = {Curran Associates, Inc.},
 title = {Movement Pruning: Adaptive Sparsity by Fine-Tuning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf},
 volume = {33},
 year = {2020}
}

@online{pruning_rt,
  author = {Jeff Pool, Abhishek Sawarkar and Jay Rodge},
  title = {Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT},
  year = 2021,
  url = {https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/},
  urldate = {2025-04-06}
}

@INPROCEEDINGS{tfmvp,
  author={Yoo, Eunji and Park, Gunho and Min, Jung Gyu and Jung Kwon, Se and Park, Baeseong and Lee, Dongsoo and Lee, Youngjoo},
  booktitle={2023 60th ACM/IEEE Design Automation Conference (DAC)}, 
  title={TF-MVP: Novel Sparsity-Aware Transformer Accelerator with Mixed-Length Vector Pruning}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  keywords={Semiconductor device modeling;Analytical models;Design automation;Transformers;CMOS technology;Energy efficiency;Hardware;Algorithm-hardware co-optimization;Model Compression;Sparsity-aware transformer accelerator},
  doi={10.1109/DAC56929.2023.10247799}
}

@misc{nvidia_sparse_tensor_core,
      title={Accelerating Sparse Deep Neural Networks}, 
      author={Asit Mishra and Jorge Albericio Latorre and Jeff Pool and Darko Stosic and Dusan Stosic and Ganesh Venkatesh and Chong Yu and Paulius Micikevicius},
      year={2021},
      eprint={2104.08378},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2104.08378}, 
}

@misc{nrx_arxiv,
Author = {Reinhard Wiesmayr and Sebastian Cammerer and Fayçal Aït Aoudia and Jakob Hoydis and Jakub Zakrzewski and Alexander Keller},
Title = {Design of a Standard-Compliant Real-Time Neural Receiver for 5G NR},
Year = {2024},
Eprint = {arXiv:2409.02912}
}

@misc{nvidia-diffusion,
  title = {NVIDIA Developer},
  author = {Zhiyu Cheng, Erin Ho and Justin Xin},
  year = {2025},
  url = {https://developer.nvidia.com/blog/tensorrt-accelerates-stable-diffusion-nearly-2x-faster-with-8-bit-post-training-quantization},
  note = {Accessed on May 30, 2025}
}

@misc{OAI,
  title = {OpenAirInterface},
  author = {Eurecom},
  year = {2025},
  url = {https://gitlab.eurecom.fr/oai/openairinterface5g},
  note = {Accessed on May 30, 2025}
}

@inproceedings{nrx_globcom,
  author={Cammerer, Sebastian and Aoudia, Fayçal Aït and Hoydis, Jakob and Oeldemann, Andreas and Roessler, Andreas and Mayer, Timo and Keller, Alexander},
  booktitle={2023 IEEE Globecom Workshops (GC Wkshps)}, 
  title={A Neural Receiver for 5G NR Multi-User MIMO}, 
  year={2023},
  volume={},
  number={},
  pages={329-334},
  keywords={Training;Time-frequency analysis;OFDM;Channel estimation;Receivers;Computer architecture;Artificial neural networks},
  doi={10.1109/GCWkshps58843.2023.10464486}
}

@article{distill,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}