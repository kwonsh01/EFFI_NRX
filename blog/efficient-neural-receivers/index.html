<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let e=e=>e.trim(),t=e=>e.innerText,n=e=>{let t=e.split(" "),n=t.slice(0,-1).join(" ");return[t.at(-1),n]},a=Array.from(document.getElementsByClassName("author")).map(t).map(e).map(n),i=a[0][0],r=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(t).map(e),"May 30, 2025"),o="Accelerating Efficient Neural Receivers for Real-Time 5G Communication: Methods and Implementation",l="Neural receivers offer significant performance benefits for 5G NR systems, but their real-time deployment is challenging due to strict URLLC latency and hardware efficiency requirements. This work introduces EffNRX, a systematically optimized neural receiver designed to overcome these limitations. We thoroughly evaluated quantization, pruning, and knowledge distillation, finding that FP8 quantization delivered the best trade-off between speed and accuracy. Our optimal configuration, EffNRX (NRX_Large with FP8 quantization and 6 CGNN iterations), achieves near state-of-the-art error correction while meeting sub-millisecond latency on commercial GPUs. Benchmarking against baselines like OAI, EffNRX demonstrates 6.08\xd7 better error-rate performance and 3.26\xd7 faster processing, proving that neural baseband processing is now practically viable for high-performance, real-time wireless communication.";{let e=a.map(e=>`${e[0]}, ${e[1]}`).join(" and "),t=`\n@inproceedings{${(i+"2025"+o.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${o}},\n  abstract = {${l}},\n  booktitle = {ICLR Blogposts 2025},\n  year = {2025},\n  date = {${r}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=t}{let e=a.map(e=>e[0]),t=`\n${e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0]}, "${o}", ICLR Blogposts, 2025.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=t}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Accelerating Efficient Neural Receivers for Real-Time 5G Communication: Methods and Implementation | ICLR Blogposts 2025</title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Neural receivers offer significant performance benefits for 5G NR systems, but their real-time deployment is challenging due to strict URLLC latency and hardware efficiency requirements. This work introduces EffNRX, a systematically optimized neural receiver designed to overcome these limitations. We thoroughly evaluated quantization, pruning, and knowledge distillation, finding that FP8 quantization delivered the best trade-off between speed and accuracy. Our optimal configuration, EffNRX (NRX_Large with FP8 quantization and 6 CGNN iterations), achieves near state-of-the-art error correction while meeting sub-millisecond latency on commercial GPUs. Benchmarking against baselines like OAI, EffNRX demonstrates 6.08× better error-rate performance and 3.26× faster processing, proving that neural baseband processing is now practically viable for high-performance, real-time wireless communication."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/EFFI_NRX/assets/img/iclr_favicon.ico"> <link rel="stylesheet" href="/EFFI_NRX/assets/css/main.css"> <link rel="canonical" href="https://kwonsh01.github.io/EFFI_NRX/blog/efficient-neural-receivers/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/EFFI_NRX/assets/js/theme.js"></script> <script src="/EFFI_NRX/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/EFFI_NRX/assets/js/distillpub/template.v2.js"></script> <script src="/EFFI_NRX/assets/js/distillpub/transforms.v2.js"></script> <script src="/EFFI_NRX/assets/js/distillpub/overrides.js"></script> <style type="text/css">d-article{overflow-x:visible}.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}[data-theme="dark"] details[open]{--bg:#112f4a;color:white;border-radius:15px;padding-left:8px;background:var(--bg);outline:.5rem solid var(--bg);margin:0 0 2rem 0;font-size:80%}.box-note{font-size:18px;padding:15px 15px 0 15px;margin:20px 20px 20px 5px;border:1px solid #eee;border-left-width:5px;border-radius:5px 5px 5px 5px}d-article .box-note{background-color:#f8fafc;border-left-color:#c80150}html[data-theme='dark'] d-article .box-note{background-color:#2e3133;border-left-color:#c80150}</style> <d-front-matter> <script async type="text/json">{
      "title": "Accelerating Efficient Neural Receivers for Real-Time 5G Communication: Methods and Implementation",
      "description": "Neural receivers offer significant performance benefits for 5G NR systems, but their real-time deployment is challenging due to strict URLLC latency and hardware efficiency requirements. This work introduces EffNRX, a systematically optimized neural receiver designed to overcome these limitations. We thoroughly evaluated quantization, pruning, and knowledge distillation, finding that FP8 quantization delivered the best trade-off between speed and accuracy. Our optimal configuration, EffNRX (NRX_Large with FP8 quantization and 6 CGNN iterations), achieves near state-of-the-art error correction while meeting sub-millisecond latency on commercial GPUs. Benchmarking against baselines like OAI, EffNRX demonstrates 6.08× better error-rate performance and 3.26× faster processing, proving that neural baseband processing is now practically viable for high-performance, real-time wireless communication.",
      "published": "May 30, 2025",
      "authors": [
        {
          "author": "Seungjun Kim",
          "authorURL": "https://sites.google.com/view/epiclab/member/sjkim",
          "affiliations": [
            {
              "name": "Pohang University of Science and Technology",
              "url": ""
            }
          ]
        },
        {
          "author": "Soonhyun Kwon",
          "authorURL": "https://sites.google.com/view/epiclab/member/shkwon",
          "affiliations": [
            {
              "name": "Pohang University of Science and Technology",
              "url": ""
            }
          ]
        },
        {
          "author": "Chanhee Lee",
          "authorURL": "https://sites.google.com/view/epiclab/member/chlee",
          "affiliations": [
            {
              "name": "Pohang University of Science and Technology",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/EFFI_NRX//">ICLR Blogposts 2025</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/EFFI_NRX/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/EFFI_NRX/call/">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/EFFI_NRX/submitting/">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/EFFI_NRX/reviewing/">reviewing</a> </li> <li class="nav-item "> <a class="nav-link" href="/EFFI_NRX/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener noopener noreferrer" target="_blank"><strong>2025</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener noopener noreferrer" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener noopener noreferrer" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener noopener noreferrer" target="_blank">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Accelerating Efficient Neural Receivers for Real-Time 5G Communication: Methods and Implementation</h1> <p>Neural receivers offer significant performance benefits for 5G NR systems, but their real-time deployment is challenging due to strict URLLC latency and hardware efficiency requirements. This work introduces EffNRX, a systematically optimized neural receiver designed to overcome these limitations. We thoroughly evaluated quantization, pruning, and knowledge distillation, finding that FP8 quantization delivered the best trade-off between speed and accuracy. Our optimal configuration, EffNRX (NRX_Large with FP8 quantization and 6 CGNN iterations), achieves near state-of-the-art error correction while meeting sub-millisecond latency on commercial GPUs. Benchmarking against baselines like OAI, EffNRX demonstrates 6.08× better error-rate performance and 3.26× faster processing, proving that neural baseband processing is now practically viable for high-performance, real-time wireless communication.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#preliminaries-and-neural-receiver">Preliminaries and Neural Receiver</a></div> <ul> <li><a href="#communication-systems">Communication Systems</a></li> <li><a href="#neural-receiver">Neural Receiver</a></li> </ul> <div><a href="#quantized-neural-receiver-everything-about-quantization-for-effinrx">Quantized Neural Receiver: Everything About Quantization for EffiNRX</a></div> <ul> <li><a href="#number-systems-two-ways-a-computer-represents-numbers">Number Systems — Two Ways a Computer Represents Numbers</a></li> <li><a href="#what-is-quantization-and-why-bother">What Is Quantization and Why Bother?</a></li> <li><a href="#quantize-nrx-for-efficient-implementation">Quantize NRX for Efficient Implementation</a></li> </ul> <div><a href="#pruned-neural-receiver-beyond-compression-toward-hardware-acceleration">Pruned Neural Receiver: Beyond Compression Toward Hardware Acceleration</a></div> <ul> <li><a href="#unstructured-vs-structured-which-pruning-approach-is-more-effective">Unstructured vs Structured: Which Pruning Approach Is More Effective?</a></li> <li><a href="#pruning-experiment-setup-for-effinrx">Pruning Experiment Setup for EffiNRX</a></li> <li><a href="#impact-of-pruning-changes-in-tbler-and-acceleration">Impact of Pruning: Changes in TBLER and Acceleration</a></li> <li><a href="#conclusion-is-pruning-an-attractive-solution-for-neural-network-based-communication-systems">Conclusion: Is Pruning an Attractive Solution for Neural Network-Based Communication Systems?</a></li> </ul> <div><a href="#distilled-neural-receiver-transferring-knowledge-for-lightweight-inference">Distilled Neural Receiver: Transferring Knowledge for Lightweight Inference</a></div> <ul> <li><a href="#distillation-strategy">Distillation Strategy</a></li> </ul> <div><a href="#optimized-neural-receiver-real-time-efficiency-at-the-edge">Optimized Neural Receiver: Real-Time Efficiency at the Edge</a></div> <ul> <li><a href="#additional-latency-analysis">Additional Latency Analysis</a></li> </ul> <div><a href="#conclusions">Conclusions</a></div> </nav> </d-contents> <h1 id="introduction">Introduction</h1> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/system_model-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/system_model-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/system_model-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/system_model.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Recent advances in deep learning have opened new possibilities for physical-layer signal processing in wireless communication systems. A typical receiver (Rx) above involves transmitting modulated signals over a wireless channel, where they are affected by noise, fading, and interference. The receiver is then responsible for recovering the original information.</p> <p>Neural Receiver (NRX)<d-cite key="nrx_globcom"></d-cite> can replace such conventional receiver with a unified model that learns directly from data. While the performance benefits of these systems have been well established, their real-world deployment in 5G New Radio (NR) environments remains limited. This gap stems primarily from the strict latency budgets imposed by Ultra-Reliable Low-Latency Communication (URLLC), which make large neural models difficult to deploy in real-time systems.</p> <p>In this blog post, we present <strong>EffNRX</strong>, a systematically optimized neural receiver designed for real-time operation. We evaluate three orthogonal model compression techniques—quantization, pruning, and knowledge distillation—and analyze their trade-offs across latency, accuracy, and memory footprint. EffiNRX paves the way for practical neural baseband processing in next-generation wireless systems.</p> <h1 id="preliminaries-and-neural-receiver">Preliminaries and Neural Receiver</h1> <h2 id="communication-systems">Communication Systems</h2> <p>In communication systems, let the symbol sequence sent by the user be denoted as $\mathbf{x}$, and the symbol sequence received by the base station be denoted as $\mathbf{y}$. The communication model is given by:</p> \[\mathbf{y}=\mathbf{Hx}+\mathbf{n}\] <p>Here, $\mathbf{H}$ is the channel matrix, and $\mathbf{n}$ is a noise vector that follows a Gaussian distribution. From the base station’s perspective, only the received signal is known. Therefore, when mapping the transmitted symbols onto a resource grid — a time-frequency grid — we additionally map known signals that both the base station and the user are aware of. Through this, the base station can observe how the known signals have been altered to estimate the channel, and by analyzing the estimated channel and the distribution of the received signals, it can determine what the transmitted symbols were. This process of estimating the channel and determining the transmitted signals is the main responsibility of the receiver’s processing, and the design of this stage directly affects the overall communication quality.</p> <h2 id="neural-receiver">Neural Receiver</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/NRX-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/NRX-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/NRX-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/NRX.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>NRX (Neural Receiver) <d-cite key="nrx_globcom"></d-cite> a fully neural uplink receiver that processes an entire 5G NR PUSCH slot in one shot, jointly performing channel estimation, equalisation and soft-bit demapping. An initial convolutional front end—three 3 × 3 depth-wise separable layers that merge the received resource-grid samples, an LS channel bootstrap and a two-dimensional positional-pilot encoding—produces a slot-wide state tensor for every active spatial layer. This tensor then flows through a fixed number of unrolled message-passing iterations. In each iteration, a tiny MLP converts every resource element’s state vector into a message; the messages coming from all other layers are averaged so the network can adapt to an arbitrary and previously unknown user count. A second separable-convolution block, equipped with residual connections and unique weights per iteration, fuses the aggregated message back into the state tensor, allowing the network to refine both the channel and the data hypotheses step by step. After the last iteration, a shared read-out MLP turns the final state vectors into bit-wise log-likelihood ratios. All told, the model’s convolutional-plus-graph design lets the same weights scale from one to four users and from four to more than two hundred PRBs without retraining.</p> <p>Training uses binary cross-entropy summed over every resource element, layer and iteration so that early stopping at any iteration still yields well-calibrated outputs. Gradient descent is performed with ADAM at a learning rate of $10^{-3}$ Each mini-batch randomises the number of active layers (drawn from a triangular distribution biased toward crowded scenes), draws a fresh 3GPP UMi channel realisation with user speeds between 0 and 34 m/s, and selects an SNR from a wide uniform range. Although the network is trained on just four PRBs, it generalises seamlessly to full-band deployments of up to 217 PRBs. Triangular sampling and broad SNR randomisation prevent over-fitting to any specific propagation profile.</p> <p>Several additional design choices are critical. A “positional-pilot” encoding supplies each time–frequency bin with its normalised distance to the nearest DMRS symbol, letting shallow convolutions exploit two-dimensional channel correlation. Feeding an LS channel estimate as a separate input accelerates convergence and helps the network disentangle layers that share the same DMRS sequence. By combining graph-style message aggregation with lightweight separable convolutions, the architecture eliminates matrix inversions and big fully connected blocks, slashing multiply-accumulate counts relative to classical K-best detection.</p> <h1 id="quantized-neural-receiver-everything-about-quantization-for-effinrx">Quantized Neural Receiver: Everything About Quantization for EffiNRX</h1> <h2 id="number-systems--two-ways-a-computer-represents-numbers">Number Systems — Two Ways a Computer Represents Numbers</h2> <p>Before diving into deep learning, we must first understand how hardware encodes and manipulates “numbers.” Modern processors usually offer two representations: integers and floating-point.</p> <ul> <li> <strong>Integers</strong> store values as two’s-complement binary within a fixed bit-width. For example, 8-bit <code class="language-plaintext highlighter-rouge">int8</code> covers −128 to 127 exactly. Because each representable value is spaced uniformly, integers are ideal for evenly distributed data. Their adders and multipliers are also far simpler than floating-point units, giving big wins in power and latency. When fractional precision is required, one can move the binary “point” inside the word (fixed-point), but every new point location forces a redesign of the entire compute path and costs extra shifters.</li> <li> <strong>Floating-point</strong> mimics scientific notation, splitting a number into a significand and an exponent. <em>IEEE-754</em> <code class="language-plaintext highlighter-rouge">fp32</code> uses 1 sign bit, 8 exponent bits, and 23 fraction bits, covering roughly $(10^{-38})~(10^{38})$Unlike integers, the spacing between values shrinks near zero and widens further away. Because neural-network weights are roughly Gaussian, they cluster around zero; dense spacing there is a huge advantage. Floating-point is therefore indispensable during training, but full 32-bit units are bulky and hot—awkward on mobile devices. Recent formats such as <code class="language-plaintext highlighter-rouge">fp16</code> or <code class="language-plaintext highlighter-rouge">fp8</code> trim the mantissa aggressively, yet they still cost more gates than integers.</li> </ul> <h2 id="what-is-quantization-and-why-bother">What Is Quantization and Why Bother?</h2> <p>Using more bits yields arithmetic that matches real-number math, but memory and logic scale linearly (or worse) with bit-width. Doubling precision rarely doubles accuracy. Quantization purposefully reduces bit-width while capping the accuracy loss. Formally, a real value $x$ is approximated by an integer $q$ via a scale $S$ and zero-point $Z$:</p> \[q \;=\; \mathrm{round}\!\left(\frac{x}{S}\right) + Z,\quad x \approx S\,(q - Z)\] <p>This brings three key benefits:</p> <ul> <li> <strong>Model size</strong>: Converting fp32 weights to <code class="language-plaintext highlighter-rouge">int8</code> (or <code class="language-plaintext highlighter-rouge">fp8</code>) cuts storage by 4×; int4 squeezes it by 8×.</li> <li> <strong>Compute &amp; power</strong>: Narrower ALUs mean fewer gates and faster clocks, slashing energy per MAC.</li> <li> <strong>Memory bandwidth</strong>: Shifting from 32-bit to 8-bit cuts DRAM↔NPU traffic by 75 %.</li> </ul> <h3 id="symmetric-vs-asymmetric-scaling">Symmetric vs Asymmetric Scaling</h3> <p>Symmetric quantization places 0 at the center of a ±max range—perfect for weight tensors whose mean is near zero. Asymmetric quantization shifts the range upward, wasting no codes on impossible negatives (e.g. ReLU activations). It needs per-multiply offset correction, so the hardware is slightly busier.</p> <h3 id="static-vs-dynamic-scaling">Static vs Dynamic Scaling</h3> <p>Static quantization (calibration): Run a held-out sample set once, fix the min/max, and reuse the scale at inference. Simple and fast, but brittle if runtime data drift. Dynamic quantization: Re-estimate min/max per batch (or via EMA). Extra overhead, yet very effective in NLP where sequence lengths vary wildly.</p> <h3 id="ptq-post-training-quantization-vs-qat-quantization-aware-training">PTQ (Post-Training Quantization) vs QAT (Quantization-Aware Training)</h3> <p>PTQ inserts Quantize/Dequantize (Q/DQ) ops after a full-precision model has converged. A few hundred calibration batches usually suffice. <code class="language-plaintext highlighter-rouge">int8</code> PTQ keeps accuracy high on both CNNs and Transformers, but at 4 bits or less the error explodes. QAT fuses “fake-quant” nodes directly into the forward pass while letting gradients flow unaltered (Straight-Through Estimator). Accuracy even at 4 bits is excellent, yet training time grows and framework support is trickier, especially for large LLMs.</p> <h2 id="quantize-nrx-for-efficient-implementation">Quantize NRX for Efficient Implementation</h2> <p>NVIDIA GPUs provide TensorRT, which maps operations like matrix multiplication onto optimized engines, assigning the right low-precision units and maximizing parallelism—indispensable for efficient DNN deployment on GPU. The existing NRX framework is TensorFlow-based, and its wireless-channel library Sionna is likewise. While PyTorch models can be quantized in place, TensorFlow ones must first be exported to ONNX and then quantized. Thus the workflow is:</p> <ol> <li>Export the TensorFlow model to <strong>ONNX</strong>.</li> <li>Quantize the ONNX graph.</li> <li>Build a TensorRT engine.</li> </ol> <p>In our experiment we wrap NRX’s positional-encoding and data-extraction blocks so that they take the initial channel estimates <code class="language-plaintext highlighter-rouge">h_hat_real/imag</code> and received symbols <code class="language-plaintext highlighter-rouge">rx_slot_real/imag</code>, and output LLRs. The wrapper is exported to ONNX, then quantized with TensorRT’s modelopt.onnx.quantization. The workflow has two key stages.</p> <h3 id="insert-explicit-q--dq-nodes">Insert Explicit Q / DQ Nodes</h3> <p>Implicit quantization via trtexec often misses layers. Instead we invoke modelopt.onnx.quantization and force Q/DQ(Quantization/Dquantization) nodes:</p> <pre><code class="language-shell=">python3 -m modelopt.onnx.quantization --onnx_path=../onnx_models/nrx_large.onnx \
        --output_path=../onnx_models/nrx_large_fp8.onnx \
        --quantize_mode=fp8 \
        --calibration_data calib.npz \
        --use_zero_point True \
        --calibration_shapes="rx_slot_real:1x1584x14x4,rx_slot_imag:1x1584x14x4,h_hat_real:1x1584x2x4,h_hat_imag:1x1584x2x4"
</code></pre> <p>The resulting <code class="language-plaintext highlighter-rouge">.onnx</code> file now contains explicit Quantize/Dequantize nodes in front of every convolution and matrix-multiplication layer, ensuring TensorRT cannot silently skip them.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/qdq-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/qdq-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/qdq-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/qdq.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>In the resulting .onnx, each convolution or matmul is now preceded by Q/DQ nodes, faithfully modeling quantization effects.</p> <h3 id="model-compile-with-tensorrt">Model compile with TensorRT</h3> <p>With Q/DQ in place, we compile the model:</p> <pre><code class="language-shell=">trtexec --fp8 \
        --stronglyTyped \
        --onnx=../onnx_models/nrx_large_fp8.onnx \
        --saveEngine=../onnx_models/nrx_large_fp8.plan \
        --minShapes=rx_slot_real:1x1584x14x4,rx_slot_imag:1x1584x14x4,h_hat_real:1x1584x2x4,h_hat_imag:1x1584x2x4 \
        --optShapes=rx_slot_real:1x1584x14x4,rx_slot_imag:1x1584x14x4,h_hat_real:1x1584x2x4,h_hat_imag:1x1584x2x4 \
        --maxShapes=rx_slot_real:1x1584x14x4,rx_slot_imag:1x1584x14x4,h_hat_real:1x1584x2x4,h_hat_imag:1x1584x2x4
</code></pre> <p>Although you can specify the quantization precision with the <code class="language-plaintext highlighter-rouge">--fp8</code> flag, TensorRT cannot correctly recognize the layers unless you also pass the <code class="language-plaintext highlighter-rouge">--stronglyTyped</code> flag to indicate that explicit quantization layers are present. This is a known issue in TensorRT 10.2. When the information is supplied correctly, TensorRT will compile the model and generate dummy inputs for testing. However, if the dummy-input dimensions are not provided accurately, layers such as Reshape inside the ONNX model will throw errors, so you must explicitly declare the input dimensions.</p> <h3 id="tbler-simulation">TBLER simulation</h3> <p>Following the baseline scenario—two users, four base-station antennas, and uplink transmission from the users to the BS—we performed the quantization described above. For <code class="language-plaintext highlighter-rouge">fp8</code> we simply used the maximum-value scaling method, whereas for <code class="language-plaintext highlighter-rouge">int8</code> we adopted AWQ. The target models are NRX_RT (CGNN iterations = 2) and NRX_Large (CGNN iterations = 8). NRX_Large achieves better TBLER thanks to its higher iteration count, but its 0.44 M parameters make it relatively large and slower at inference. NRX_RT (Real-Time) <d-cite key="nrx_arxiv"></d-cite> delivers lower performance than NRX_Large because it uses only two iterations, yet it is efficient with just 0.14 M parameters and enjoys faster inference. Moreover, it satisfies the URLLC latency requirement of &lt; 1 ms. Performance is evaluated by TBLER (Transmit Block Error Rate), i.e., the fraction of transmitted blocks that contain errors; TBLER generally decreases as noise power is reduced</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/Quant_TBLER-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/Quant_TBLER-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/Quant_TBLER-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/Quant_TBLER.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>In the figure, the left graph shows RT and the right graph shows Large. To benchmark quantization, we plotted three additional curves:</p> <ul> <li>LSlin + LMMSE – LS (Least Square) channel estimation with LMMSE (Linear Minimum Mean Square Error) detection; the most basic receiver.</li> <li>LMMSE + K-best – LMMSE channel estimation with K-best detection; powerful performance but extremely high complexity due to complex operations such as matrix inversion.</li> <li>Perf. CSI + K-best – K-best detection with perfect CSI; effectively ground truth.</li> </ul> <p>For RT, baseline <code class="language-plaintext highlighter-rouge">fp16</code> is a little worse than LMMSE + K-best. Applying <code class="language-plaintext highlighter-rouge">fp8</code> quantization causes a noticeable drop yet still outperforms the basic receiver, whereas <code class="language-plaintext highlighter-rouge">int8</code> quantization degrades performance to an unusable level. This implies that RT’s parameter distribution is heavily concentrated near the zero-point with high variance and outliers, making <code class="language-plaintext highlighter-rouge">fp8</code> quantization more effective than <code class="language-plaintext highlighter-rouge">int8</code>. In the Large model, the <code class="language-plaintext highlighter-rouge">fp16</code> curve is already slightly better than LMMSE + K-best. <code class="language-plaintext highlighter-rouge">fp8</code> introduces only a tiny loss and still beats LMMSE + K-best; <code class="language-plaintext highlighter-rouge">int8</code> loses a bit more and ends up roughly equal to LMMSE + K-best. Because Large has far more parameters than RT, its accuracy is less sensitive to quantization. In both cases, FP8 surpasses <code class="language-plaintext highlighter-rouge">int8</code> thanks to the weight-distribution characteristics.</p> <h3 id="throughput-and-latency-anaysis">Throughput and Latency anaysis</h3> <p>As noted earlier, achieving a low error rate is critical; however, latency is equally vital—if processing exceeds the time budget, the communication protocol collapses. We therefore analyzed latency on an RTX 6000 Ada.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/cost_table-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/cost_table-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/cost_table-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/cost_table.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>For RT, <code class="language-plaintext highlighter-rouge">fp32</code> exceeds the URLLC 1 ms budget (1.173 ms), but <code class="language-plaintext highlighter-rouge">fp16</code>, <code class="language-plaintext highlighter-rouge">fp8</code>, and <code class="language-plaintext highlighter-rouge">int8</code> all meet it. The baseline paper, run on an A100, reported 1 ms for RT_FP16; our numbers differ because we use the Ada Lovelace rather than the Ampere architecture—details to follow. Relative to <code class="language-plaintext highlighter-rouge">fp16</code>, <code class="language-plaintext highlighter-rouge">fp8</code> and <code class="language-plaintext highlighter-rouge">int8</code> achieve speed-ups of <em>×1.27</em> and <em>×1.13</em>, respectively. Large shows similar trends:<code class="language-plaintext highlighter-rouge">fp8</code> and <code class="language-plaintext highlighter-rouge">int8</code> deliver ×1.33 and ×1.11 speed-ups over <code class="language-plaintext highlighter-rouge">fp16</code>. TensorRT 10.2’s <code class="language-plaintext highlighter-rouge">fp8</code> quantization focuses mainly on MatMul, so it is not ideal for NRX, which relies heavily on convolutions. All fully connected layers were quantized, but in separable convolutions only the pointwise-convolution (channel-wise fully connected) part was quantized; depthwise convolutions were not.</p> <h3 id="why-is-fp8-faster-than-int8-even-though-both-use-8-bits">Why is FP8 faster than INT8 even though both use 8 bits?</h3> <blockquote> <ul> <li> <code class="language-plaintext highlighter-rouge">fp8</code> needs no type conversion during accumulation.</li> <li> <code class="language-plaintext highlighter-rouge">int8</code> incurs dequantization/requantization overhead to <code class="language-plaintext highlighter-rouge">fp16</code>.</li> <li>On Tensor Cores, <code class="language-plaintext highlighter-rouge">fp8</code> operations can be fused into a single kernel.</li> </ul> </blockquote> <aside class="l-body box-note"> <p>For these reasons, <code class="language-plaintext highlighter-rouge">fp8</code> enables faster NRX inference than <code class="language-plaintext highlighter-rouge">int8</code>—an effect that generalizes to other models. For example, with Diffusion XL 1.0, <code class="language-plaintext highlighter-rouge">int8</code> and <code class="language-plaintext highlighter-rouge">fp8</code> achieve ×1.72 and ×1.95 speed-ups over <code class="language-plaintext highlighter-rouge">fp16</code>, respectively. <d-cite key="nvidia-diffusion"></d-cite></p> <p>In summary, <code class="language-plaintext highlighter-rouge">fp8</code> quantization minimizes accuracy loss while offering the best speed-up, but even so, none of the precisions fully satisfy every URLLC requirement, so additional compression techniques or model modifications are still needed.</p> </aside> <h1 id="pruned-neural-receiver-beyond-compression-toward-hardware-acceleration">Pruned Neural Receiver: Beyond Compression Toward Hardware Acceleration</h1> <p>Despite their impressive performance, modern deep learning models often require substantial computational resources and memory due to their massive number of parameters. This becomes a significant obstacle in resource constrained environments such as mobile devices and edge platforms. One effective approach to address this issue is pruning, a technique that selectively removes less important parameters within a neural network to reduce model size and improve computational efficiency.</p> <p>The core idea of pruning goes beyond merely removing unnecessary parameters it lies in discovering an optimal subnetwork within a larger model. This concept is closely tied to the well known Lottery Ticket Hypothesis<d-cite key="pruning"></d-cite>. According to this hypothesis, large neural network contains smaller subnetworks referred to as <strong><em>winning lottery tickets</em></strong> that can achieve comparable or even superior performance to the original network. Thus, the pruning process can be viewed not just as a form of compression, but as a means of uncovering a hidden, efficient architecture embedded within the overparameterized model.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/pruning-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/pruning-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/pruning-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/pruning.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Therefore, pruning is a process that increases the sparsity of models by removing redundant parameters from large networks without significantly compromising accuracy. By identifying and preserving subnetworks that maintain or even surpass the performance of original model, pruning allows neural networks to become lighter and faster while retaining their effectiveness. Through this technique, sparsity is improved with minimal degradation in performance, making the models more suitable for deployment on resource-constrained devices. The main advantages of applying pruning are as follows:</p> <ol> <li> <strong>Model Size Reduction</strong>: By removing unimportant parameters, pruning can significantly reduce the storage requirements of a model. When combined with formats like CSR (Compressed Sparse Row) or CSC (Compressed Sparse Column)—which are efficient for storing sparse matrices—models can avoid storing unnecessary weights. This is especially valuable in memory-constrained environments such as embedded systems. However, since sparse formats require additional metadata (e.g., indices), their benefits are typically realized only when the sparsity exceeds 50%.</li> <li> <strong>Hardware Acceleration</strong>: Pruned weights are effectively zero, meaning the corresponding multiplications and additions can be skipped during computation. Furthermore, fewer weights need to be loaded from memory, which helps alleviate the memory bandwidth bottleneck that often limits performance in large-scale neural networks. These advantages make it possible to leverage hardware optimized for sparse computation—such as sparse engines or specialized accelerators—to achieve significantly faster inference.</li> <li> <strong>Potential for Improved Generalization:</strong>: By eliminating unnecessary parameters, pruning encourages the model to focus on learning only the most essential features from the data. This reduction in complexity can help mitigate overfitting, especially in overparameterized networks, and in some cases, even lead to improved validation performance. Pruning thus not only compresses the model but can also act as an implicit regularizer, enhancing its ability to generalize to unseen data.</li> </ol> <p>A common strategy in pruning methods is to retain only the top $v\%$ of weights where $v = 1 - \text{sparsity}$ based on a scoring criterion assigned to each weight. We define the $Top_v$ function as a selector that preserves the weights corresponding to the top $v%$ values in the score matrix $S$.</p> \[\text{Top}_v(\mathbf{S})_{i,j} = \begin{cases} 1, &amp; \text{if } S_{i,j} \text{ is in top } v\% \\ 0, &amp; \text{o.w.} \end{cases}\] <h2 id="unstructured-vs-structured-which-pruning-approach-is-more-effective">Unstructured vs Structured: Which Pruning Approach Is More Effective?</h2> <p>In this way, pruning has established itself as a powerful technique that goes beyond simple model compression enabling the construction of more efficient and lightweight models while also facilitating hardware acceleration and performance improvement. In the following sections, we provide a detailed introduction to the two main pruning approaches: Unstructured Pruning and Structured Pruning.</p> <h3 id="unstructured-pruning">Unstructured Pruning</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/Unstructed-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/Unstructed-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/Unstructed-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/Unstructed.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Unstructured pruning removes individual weights based on their importance typically determined by their magnitude. For instance, weights with absolute values below a certain threshold can be pruned away. This approach allows for the highest levels of sparsity with minimal performance degradation, making it one of the most effective methods for reducing model size. However, because the resulting sparsity is irregular and scattered, it is difficult to exploit for acceleration on standard dense matrix hardware. Leveraging unstructured pruning for runtime speedup usually requires specialized sparse matrix libraries or dedicated sparse engines. Common methods for unstructured pruning include magnitude pruning and movement pruning<d-cite key="movement_pruning"></d-cite>.</p> <p>The image shown illustrates a hypothetical 0.5 sparsity unstructured pruning mask applied to a (16 X 16) weight matrix. As can be seen, the pruned weights are distributed randomly without any fixed pattern.</p> <ul> <li>Magnitude Pruning Magnitude pruning is one of the most intuitive and widely used pruning techniques. In this method, weights are pruned after model training, based on the magnitude of each weight. Specifically, weights with small absolute values are considered less important and are removed under the assumption that <em>weights with smaller magnitudes contribute less to the model’s output, so removing them will not significantly degrade performance.</em> This approach is officially supported by the <code class="language-plaintext highlighter-rouge">tensorflow_model_optimization</code> package, making it accessible and easy to implement. While simple and effective in many cases, magnitude pruning has its limitations. A small weight at one point in training might later become critical to model performance. Therefore, relying solely on magnitude can sometimes lead to sub-optimal pruning results, especially if done without careful scheduling or fine-tuning. The score $S$ for magnotude pruning is defined as follows:</li> </ul> \[S_{i,j} = abs(w_{i,j})\] <ul> <li>Movement Pruning<d-cite key="movement_pruning"></d-cite> Movement pruning was proposed to overcome the limitations of magnitude pruning. Rather than relying solely on the current magnitude of weights, this method evaluates weight importance based on how much a weight changes during training. Unlike magnitude pruning, which only considers the absolute value of a weight, movement pruning tracks the trajectory of each weight during training—especially during the fine-tuning phase. If a weight consistently moves away from zero, it is considered important for learning. Conversely, even if a weight has a large magnitude, it may be deemed unimportant if it trends toward zero or shows little change throughout training. The measure of this “movement” can be based purely on the weight update history or incorporate gradient information from the loss function. By capturing a weight’s dynamic contribution during training rather than just its static value, movement pruning provides a more nuanced assessment of importance. This often leads to more optimal pruning results and better model performance compared to magnitude-based methods. However, since it requires tracking weight updates over time, its implementation is more complex and incurs a higher computational cost than magnitude pruning. The score $S$ at $T$ step for movement pruning is defined as follows:</li> </ul> \[S_{i,j}^{(T)} = -\alpha_S \sum_{t &lt; T} \left( \frac{\partial \mathcal{L}}{\partial W_{i,j}} \right)^{(t)} W_{i,j}^{(t)}\] <p>Both of these methods magnitude and movement pruning are forms of unstructured pruning, which results in sparse masks with irregular patterns. While effective at reducing model size, this irregularity makes it difficult to accelerate inference on most hardware. To address this, a different class of methods called structured pruning introduces regular sparsity patterns that enable hardware-friendly acceleration.</p> <h3 id="structured-pruning">Structured Pruning</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/Structed-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/Structed-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/Structed-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/Structed.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Common Method Structured pruning removes groups of weights such as entire channels, filters, or neurons instead of individual connections. While this approach may achieve lower sparsity compared to unstructured pruning, however this makes it highly hardware-friendly and easier to integrate with existing deep learning frameworks. In Transformer-based large language models (LLMs) like GPT, pruning can be applied to vector structures, leveraging the inherent structure of weights. The weights of pre-trained Transformers often exhibit vectorized patterns, making vector-structured pruning naturally aligned with the underlying architecture. Studies have shown that combining vector-level structured pruning with hardware optimized for vector utilization can lead to significantly improved throughput<d-cite key="tfmvp"></d-cite>.</p> </li> <li> <p>2:4 Method</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/24_Structed-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/24_Structed-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/24_Structed-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/24_Structed.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Recent commercial GPU architectures have increasingly incorporated dedicated hardware support for acceleration through pruning. A prominent example is NVIDIA’s Ampere architecture, which introduces a specialized hardware component known as the <strong><em>Sparse Tensor Core</em></strong>.<d-cite key="nvidia_sparse_tensor_core"></d-cite> According to the NVIDIA Developer Blog post titled “Accelerating Inference with Sparsity Using Ampere and TensorRT”<d-cite key="pruning_rt"></d-cite>, the Ampere architecture improves performance by exploiting a 2:4 sparsity pattern during matrix multiplication operations. This pattern requires that within every group of four consecutive elements, only two can be non-zero. In other words, for a tensor to qualify for sparse acceleration, each 4-element block must contain at most two non-zero values, enabling the hardware to skip redundant computations and achieve faster inference. NVIDIA GPUs equipped with <strong><em>Sparse Tensor Cores</em></strong> such as A100 and RTX 6000Ada can detect this pattern and skip computations involving zero values, executing operations only on the effective non-zero elements. This allows for up to 2× improvement in throughput, making inference significantly more efficient. The technology is integrated with NVIDIA’s deep learning inference optimization library, <code class="language-plaintext highlighter-rouge">TensorRT</code>, enabling developers to easily take advantage of sparse acceleration. Using packages like <code class="language-plaintext highlighter-rouge">tensorflow_model_optimization</code>, developers can convert pre-trained model weights to the 2:4 sparsity pattern and optimize them with TensorRT for accelerated inference on NVIDIA Ampere and newer GPUs.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/sparsity-improvements-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/sparsity-improvements-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/sparsity-improvements-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/sparsity-improvements.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <d-footnote>Sparsity improvements in performance and power efficiency (with dense as a baseline) https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/</d-footnote> <p>It has been shown that applying 2:4 pruning to ResNeXt can improve performance per watt by over 20%, while also achieving more than a 10% reduction in latency.</p> <h2 id="pruning-experiment-setup-for-effinrx">Pruning Experiment Setup for EffiNRX</h2> <p>Pruning of the Neural Receiver was conducted in a <code class="language-plaintext highlighter-rouge">TensorFlow</code> environment. The <code class="language-plaintext highlighter-rouge">tensorflow_model_optimization</code> package provides various model optimization solutions for models built with <code class="language-plaintext highlighter-rouge">TensorFlow</code>, including support for pruning. However, since the package only supports the most basic method <strong>magnitude pruning</strong> we applied pruning to the Neural Receiver using the <code class="language-plaintext highlighter-rouge">prune_low_magnitude</code> method followed by fine-tuning. Pruning was applied only to the weights, excluding the biases. The <code class="language-plaintext highlighter-rouge">prune_low_magnitude</code> method was used to prune all weights within the <code class="language-plaintext highlighter-rouge">Dense</code> and <code class="language-plaintext highlighter-rouge">SeparableConv2D</code> layers that make up the Neural Receiver. Since biases were not pruned, the overall model sparsity is slightly lower than the target sparsity.</p> <p>Both Neural Receiver Large and RT were evaluated using <strong>0.8 sparsity unstructed magnitude pruning</strong> and <strong>2:4 structed magnitude pruning</strong>, allowing for a comprehensive assessment of model performance and pruning effectiveness under each approach.</p> <p>The mask $M$ generated by applying $Top_v$ to the score matrix $S$ is used to perform an element-wise product with the weights, resulting in pruned weights that are then used in the layer’s computations.</p> \[\mathbf{A} = (\mathbf{W} \circ \mathbf{M}) \mathbf{X}\] <p>In pruning with <code class="language-plaintext highlighter-rouge">tensorflow_model_optimization</code>, pruning_config 1) was applied to achieve 0.8 sparsity pruning, while the pruning_config 2) was used for 2:4 structured pruning.</p> <pre><code class="language-python="># 1) Unstructed Pruning
pruning_config = {
    'pruning_schedule': pruning_schedule.PolynomialDecay(
        initial_sparsity=0.0,
        final_sparsity=0.8,
        begin_step=0,
        end_step=10000,
        power=1
    )
}

# 2) 2:4 Pruning
pruning_config = {
    'sparsity_m_by_n': (2, 4),
}
</code></pre> <p>The specified <code class="language-plaintext highlighter-rouge">pruning_config</code> is applied to the pruning layer instances, successfully enabling the application of the target pruning method.</p> <pre><code class="language-python="># Magnitude Pruning Layer
# layer = Dense or SeparableConv2D
layers = prune_low_magnitude(layer(d_s, (k, k), activation=None or relu, padding='same', dtype=dtype, use_bias=en_bias), **pruning_config)
</code></pre> <p>Fine-tuning for both NRX RT and Large models was conducted under identical conditions, using only 1/1000 of the epochs used during model pre-training.</p> <pre><code class="language-shell=">trtexec --sparsity=enable
</code></pre> <p>By enabling the corresponding option, <code class="language-plaintext highlighter-rouge">TensorRT</code> is able to analyze the given weights and apply appropriate sparse features to enable acceleration.</p> <h2 id="impact-of-pruning-changes-in-tbler-and-acceleration">Impact of Pruning: Changes in TBLER and Acceleration</h2> <p>After performing pruning in <code class="language-plaintext highlighter-rouge">TensorFlow</code>, we simulated the TBLER of the pruned NRX across an $E_b/N_0$ range of −2 to 7 dB. In the context of $E_b/N_0$ which represents the signal power, a lower TBLER at the same $E_b/N_0$ indicates a more powerfull model.</p> <p>We also measured GPU latency in a <em>float16</em> environment using <code class="language-plaintext highlighter-rouge">TensorRT</code>. In our experiments, even though we applied 2:4 structured pruning to enable <strong><em>Sparse Tensor Core</em></strong>, we observed that some layers were still executed using dense engines.</p> <p>Specifically, the <code class="language-plaintext highlighter-rouge">Dense</code> and depthwise components of <code class="language-plaintext highlighter-rouge">SeparableConv2D</code> layers in the NRX were always executed with the dense engine. For NRX RT, these layers have dimensions of (64 X 56), which are relatively small. In such cases, the overhead introduced by <strong><em>Sparse Tensor Core</em></strong> outweighed its benefits, leading to performance degradation compared to dense execution. More detailed experiments have shown that <strong><em>Sparse Tensor Core</em></strong> is highly effective in layers with large weight dimensions <d-cite key="nvidia_sparse_tensor_core"></d-cite>, where the benefits of sparsity outweigh the overhead. The same behavior was observed in NRX Large, which simply increases the number of CGNN iterations and therefore retains the same small dimensional layers.</p> <h3 id="nrx-rt">NRX RT</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/TBLER_pRT-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/TBLER_pRT-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/TBLER_pRT-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/TBLER_pRT.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The figure above shows the TBLER simulation results for NRX RT, including comparisons among the baseline, unstructured pruned, and 2:4 structured pruned models. Although the performance is inferior to traditional mathematical models like LMMSE + K-best, the NRX RT still <strong>greatly surpass LS + LMMSE</strong> in terms of error correction capability. However, regardless of the pruning method used, <strong>pruned NRX RT exhibits a consistent performance degradation of around 2 dB</strong>. This result stands in stark contrast to the findings observed in the NRX Large model, which demonstrates a different trend.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/pruning_size_latency_RT-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/pruning_size_latency_RT-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/pruning_size_latency_RT-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/pruning_size_latency_RT.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>As expected, although pruning was conducted with a target sparsity of 0.8, the actual achieved sparsity was slightly lower around 0.66 due to the exclusion of biases from pruning. Nevertheless, the pruned weights, when compressed using the CSR (Compressed Sparse Row) format, resulted in a <strong>significant reduction in the weight file size</strong>.</p> <p>The GPU latency measured using <code class="language-plaintext highlighter-rouge">TensorRT</code> showed almost no difference across the three cases. This aligns with our previous observation: unstructured pruning cannot leverage sparse features for acceleration, while in the case of 2:4 structured pruning, the layer dimensions are too small to benefit significantly. The overhead associated with sparse execution outweighs its advantages at such scales, resulting in <strong>negligible latency improvement</strong>.</p> <h3 id="nrx-large">NRX Large</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/TBLER_pLarge-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/TBLER_pLarge-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/TBLER_pLarge-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/TBLER_pLarge.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>In the case of NRX Large, unlike NRX RT, pruning resulted in minimal performance degradation. This is likely because NRX Large is overparameterized compared to NRX RT, containing greater redundancy, which makes it more resilient to pruning. However, the extent of performance loss varied depending on the pruning method used. In particular, the 2:4 structured pruning showed more significant degradation. This is because the structural constraint imposed for hardware acceleration prevents optimal selection of weights to prune. If a group of four weights contains three important ones, one must still be pruned to satisfy the 2:4 pattern, inevitably leading to a drop in performance.</p> <p>As highlighted in the Lottery Ticket Hypothesis<d-cite key="pruning"></d-cite>, we observed that the NRX Large model, despite having fewer total parameters after unstructured pruning compared to NRX RT, achieved superior TBLER performance. This demonstrates that identifying partial <strong><em>Winning lottery tickets</em></strong>is also feasible within neural network based communication models.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/pruning_size_latency_Large-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/pruning_size_latency_Large-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/pruning_size_latency_Large-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/pruning_size_latency_Large.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Similarly, the weight file size was significantly reduced in the unstructured pruning case, while the 2:4 structured pruning showed limited compression benefits due to its relatively low sparsity, making it difficult to exploit the advantages of CSR formatting. GPU latency results were consistent with those of NRX RT, showing negligible differences across the configurations.</p> <h2 id="conclusion-is-pruning-an-attractive-solution-for-neural-network-based-communication-systems">Conclusion: Is Pruning an Attractive Solution for Neural Network-Based Communication Systems?</h2> <aside class="l-body box-note"> <p>In conclusion, the impact of pruning observed in this study was minimal, with benefits largely limited to modest reductions in storage requirements. Particularly in terms of key performance metrics for real-world communication systems such as latency and accuracy, pruning introduced overhead from fine-tuning without delivering meaningful gains. As such, applying pruning to NRX models appears to be impractical, and it is more reasonable to avoid it in deployment or optimization scenarios. Compared to quantization, pruning offered little to no advantage. Instead, exploring alternative strategies such as architectural redesign, hardware-aware optimization, or knowledge distillation may provide more substantial efficiency improvements.</p> </aside> <h1 id="distilled-neural-receiver-transferring-knowledge-for-lightweight-inference">Distilled Neural Receiver: Transferring Knowledge for Lightweight Inference</h1> <p>Knowledge Distillation is a model compression technique that transfers the learned behavior of a large, accurate <strong>teacher</strong> model to a smaller <strong>student</strong> model<d-cite key="distill"></d-cite>. Instead of training the student solely on ground-truth labels, it learns to mimic the soft targets—typically, the probabilistic outputs or logits—generated by the teacher. These soft targets contain richer information such as uncertainty and inter-class similarity, enabling the student to generalize better. NRX faces a trade-off between performance and real-time inference latency. While NRX_Large model offers high performance, its inference latency exceeds 1 ms, making it impractical for URLLC applications. Therefore, lightweight models such as NRX_RT are required in latency-critical scenarios.</p> <h2 id="distillation-strategy">Distillation Strategy</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/distill_draw-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/distill_draw-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/distill_draw-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/distill_draw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>In our setting:</p> <ul> <li> <strong>Teacher</strong>: NRX_Large (accurate but slow)</li> <li> <strong>Student</strong>: NRX_RT (fast but less accurate)</li> <li> <strong>Distillation Target</strong>: Log-Likelihood Ratio (LLR) vectors output by the teacher</li> </ul> <p>The training objective encourages the student to align its LLR predictions with those of the teacher while also maintaining accuracy on ground-truth bits. Formally, the distillation loss can be weighted combination of:</p> <ul> <li>Binary Cross-Entropy (BCE) with true intformation sequence bits</li> <li>Mean Squared Error (MSE) between teacher and student outputs, including LLR and channel coefficient distributions</li> </ul> <p>In our experiments, the most significant performance improvement was observed when knowledge distillation was applied using only the LLR MSE loss. This is likely because the evaluation metric, TBLER is computed based on the LLRs fed into the FEC decoder. As a result, directly aligning the student’s LLR outputs with those of the teacher proved to be the most effective way to improve basic end-to-end NRX system performance.</p> <p>Knowledge distillation can be controlled using the following equation:</p> \[\text{Loss} = \alpha \cdot \text{Loss}_\text{student} + (1-\alpha) \cdot \text{Loss}_\text{distillation}\] <p>Here, $\alpha \in [0, 1]$ is a weighting factor that controls student’s loss and the distillation loss based on the teacher’s outputs. A lower alpha means the student relies more on the teacher’s guidance.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/distill_loss-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/distill_loss-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/distill_loss-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/distill_loss.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The figure above shows the training loss curves for different values of $\alpha$ for 0.1, 0.5, and 0.9 from the top.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/distill_ber-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/distill_ber-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/distill_ber-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/distill_ber.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <aside class="l-body box-note"> <p>The figure abobe shows the performance of the distilled NRX RT model, where the teacher model is NRX Large and the student model is NRX RT. We observe that the distilled model, <strong>RT Distill</strong>, achieves approximately 0.2 dB gain over the baseline NRX RT model. This demonstrates that knowledge distillation from the larger NRX Large model effectively improves the performance of the lightweight NRX_RT model without increasing inference latency.</p> <p>Although the current experiment did not combine distillation with other compression methods due to time constraints, the approach is inherently compatible with them. Future work will explore integrating distillation with quantized or pruned models to achieve further reductions in inference cost while preserving or even improving accuracy.</p> </aside> <h1 id="optimized-neural-receiver-real-time-efficiency-at-the-edge">Optimized Neural Receiver: Real-Time Efficiency at the Edge</h1> <p>Based on the results above, the take-aways for Quantization, Pruning, and Knowledge Distillation can be summarized as follows:</p> <ul> <li> <strong>Quantization</strong>: Using <code class="language-plaintext highlighter-rouge">fp8</code> improves latency with only a modest performance loss. However, on NRX Large the system still falls slightly short of the URLLC requirement, so additional lightweighting is necessary.</li> <li> <strong>Pruning</strong>: Brings virtually no latency improvement and introduces a noticeable performance drop.</li> <li> <strong>Knowledge Distillation</strong>: Boosts accuracy while keeping the RT-class computational cost, but still does not reach the performance of NRX Large.</li> </ul> <p>Accordingly, the optimal choice is to run NRX Large (not NRX RT) with <code class="language-plaintext highlighter-rouge">fp8</code> quantization and no pruning, although further lightweighting is still required. To achieve this, we chose to vary the number of CGNN iterations. Reducing CGNN iterations trades a small accuracy loss for lower latency, so we swept the iteration count while applying the best-performing quantization/pruning/distillation strategies identified earlier. The search revealed a new optimum: NRX Large with CGNN iterations set to 6 and <code class="language-plaintext highlighter-rouge">fp8 </code>quantization. For an external comparison, we consulted <strong>OAI (OpenAirInterface)</strong> <d-cite key="OAI"></d-cite>, an ongoing Eurocom project that already demonstrates reliable over-the-air communication. Because this CPU-based framework has been field-validated and commercially deployed, it serves as a trustworthy benchmark.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/TBLER_v2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/TBLER_v2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/TBLER_v2-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/TBLER_v2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>In the graph above, the label EffML marks our optimal configuration. Relative to the original NRX Large, <strong>EffNRX</strong> incurs only a 0.3 dB loss at the same TBLER, essentially matching the performance of the LMMSE + K-best receiver.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/efficiency-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/efficiency-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/efficiency-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/efficiency.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The accompanying latency-versus-TBLER plot shows that every Large-model variant fails the URLLC requirement and is therefore impractical for live deployments. The four models that do satisfy URLLC—RT_FP16, RT_FP8, and the OAI baselines—are suitable for real-world use. Among them, EffNRX delivers the best error-rate performance, recording an error rate that is 6.08 × lower than OAI’s at $E_b/N_0$ = 4 dB. It also achieves 3.26 × faster processing than the best-performing Large_FP16 model. These results demonstrate that, by carefully optimizing the model and mapping it efficiently onto a GPU, one can deliver real-time, high-performance wireless communication without an expensive dedicated modem—provided a capable GPU is available.</p> <h2 id="comparison-in-image-communication-applications">Comparison in Image Communication Applications</h2> <p>We simulated how image transmission in a wireless communication environment is affected by error rates, comparing a baseline model with our proposed model. Errors in communication manifest as incorrect pixel values in the received image.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/Image_BER-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/Image_BER-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/Image_BER-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/Image_BER.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <aside class="l-body box-note"> <p>Our model consistently demonstrates a lower error rate compared to the baseline, resulting in a cleaner image. This difference becomes even more pronounced as the signal strength increases, highlighting the superior performance of our approach.</p> </aside> <h2 id="additional-latency-analysis">Additional Latency Analysis</h2> <p>We conducted additional experiments under conditions as consistent as possible with those of the previous work <d-cite key="nrx_arxiv"></d-cite>. The focus was on evaluating whether latency improvements could be achieved through quantization and pruning techniques on both the NVIDIA A100 and RTX 6000 Ada GPUs.</p> <p>For quantization, we examined performance using FP16, FP8, and INT8 precision. In parallel, we applied both Unstructured and 2:4 Structured pruning methods to analyze latency trends across combinations.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/6000ada_vs_A100-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/6000ada_vs_A100-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/6000ada_vs_A100-1400.webp"></source> <img src="/EFFI_NRX/assets/img/2025-05-30-efficient-neural-receivers/6000ada_vs_A100.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Across all scenarios, pruning—regardless of type—did not lead to any meaningful improvements in inference latency. In contrast, quantization yielded clear latency reductions on the RTX 6000 Ada, with performance improving in the order of FP8, INT8, then FP16. However, since the A100 does not officially support FP8, that configuration was excluded from its experiments.</p> <p>Interestingly, the RTX 6000 Ada consistently outperformed the A100 in latency by approximately 1.7× across all cases. This is primarily because NRX, unlike Transformer-based large language models, has relatively few parameters and thus does not fully leverage the A100’s HBM memory bandwidth advantage. Instead, the greater core count of the RTX 6000 Ada proves more beneficial for models like NRX.</p> <p>As such, the RTX 6000 Ada offers a more cost-effective and latency-efficient solution compared to the A100 for deploying neural receivers.</p> <h1 id="conclusions">Conclusions</h1> <p>In this work, we explored the design and optimization of a neural receiver (NRX) suitable for real-time operation in 5G NR systems. Despite the well-established performance benefits of neural receivers, their practical deployment has been limited due to strict URLLC latency constraints and hardware efficiency concerns. To address these challenges, we proposed EffNRX, a streamlined and systematically optimized neural receiver architecture. We evaluated three key model compression techniques—quantization, pruning, and knowledge distillation—to understand their impact on latency, accuracy, and hardware efficiency:</p> <ul> <li>Quantization, particularly FP8, offered the best trade-off by significantly reducing inference time with minimal performance degradation.</li> <li>Pruning, both unstructured and 2:4 structured, resulted in negligible latency improvement and caused noticeable drops in accuracy, making it less suitable for real-time deployment in NRX systems.</li> <li>Knowledge distillation effectively improved the performance of lightweight models like NRX_RT without increasing latency, demonstrating its value as a complementary technique to quantization.</li> </ul> <p>Based on extensive experiments, we identified an optimal configuration: NRX_Large with FP8 quantization and a reduced number of CGNN iterations (e.g., 6). This configuration—EffNRX—achieves near state-of-the-art error correction performance while satisfying sub-millisecond latency requirements on commercial GPUs like the RTX 6000 Ada.</p> <p>Furthermore, we benchmarked EffNRX against baselines such as OAI and conventional receivers (e.g., LMMSE + K-best), demonstrating 6.08× better error-rate performance and 3.26× faster processing compared to the best-performing FP16 variant. These findings confirm that neural baseband processing is now not only theoretically appealing but also practically viable with careful design and deployment strategies.</p> <p>Looking ahead, combining distillation with quantized inference, exploring hardware-aware architecture redesign, and leveraging emerging GPU acceleration features such as sparse tensor cores will be key to further pushing the boundaries of efficient neural signal processing for 6G and beyond.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/EFFI_NRX/assets/bibliography/2025-05-30-efficient-neural-receivers.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2025" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>